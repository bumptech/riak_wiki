
<div id="toc"></div>
h2. Summary

Riak is a distributed database, strongly influenced by the [[Dynamo Paper|http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf]] and the [[CAP Theorem|http://portal.acm.org/citation.cfm?doid=564585.564601]].  It supports high availability by allowing tunable levels of guarantees for durability and eventual&nbsp;consistency.

A Riak cluster is generally run on a set of well-connected physical hosts.  Each host in the cluster runs one Riak node.  Each Riak node runs a set of virtual nodes, or "vnodes", that are each responsible for storing a separate portion of the key space.

Nodes are not clones of each other, nor do they all participate in fulfilling every request.  The extent to which data is replicated, and when, and with what merge strategy and failure model, is configurable at runtime.

h2. The Ring

Much of this section is discussed in the Dynamo paper, but it's a good summary of how Riak implements the necessities.

Riak's client interface speaks of [[buckets|Riak Glossary#Bucket]] and [[keys|Riak Glossary#Key]].  Internally, Riak computes a 160-bit binary hash of the bucket/key pair, and maps this value to a position on an ordered "ring" of all such values. This ring is divided into partitions.  Each Riak vnode is responsible for a partition (we say that it "claims" that partition).

The nodes of a Riak cluster each attempt to run roughly an equal number of vnodes.  In the general case, this means that each node in the cluster is responsible for 1/(number of nodes) of the ring, or (number of partitions)/(number of nodes) vnodes.  For example, if two nodes define a 16-partition cluster, then each node will run 8 vnodes. Nodes claim their partitions at random intervals around the ring, in an attempt at even distribution.


<div class="note">Currently, the distribution of partitions among the nodes of the cluster may result in object replicas not being placed on N distinct nodes. This issue has been reported and discussed in [[Bug #228|https://issues.basho.com/show_bug.cgi?id=228]]. The discussion includes an operational workaround that involves setting the "riak_core" parameter "target_n_val" to "4" to ensure object replicas will be placed on distinct nodes when N=3. More information is available in the comments section of the bug.</div>


When a value is being stored in the cluster, any node may participate as the coordinator for the request.  The coordinating node consults the ring state to determine which vnode owns the partition in which the value's key belongs, then sends the "put" request to that vnode, as well as the vnodes responsible for the next N-1 partitions in the ring, where N is a bucket-configurable parameter that describes how many copies of the value to store.  The put request may also specify that at least W (=< N) of those vnodes reply with success, and that DW (=< W) reply with success only after durably storing the value.

A fetch, or "get", request operates similarly, sending requests to the vnode that "claims" the partition in which the key resides, as well as to the next N-1 partitions.  The request also specifies R (=< N), the number of vnodes that must reply before a response is returned.

h2. Gossiping

The ring state is shared around the cluster by means of a "gossip protocol".  Whenever a node changes its claim on the ring, it announces its change via this protocol.  It also periodically re-announces what it knows about the ring, in case any nodes missed previous updates.

h2. Vector clocks

With any node able to drive any request, and not all nodes needing to participate in each request, it is necessary to have a method for keeping track of which version of a value is current.  This is where vclocks come in.  The vector clocks used in Riak are based on the [[work of Leslie Lamport|http://portal.acm.org/citation.cfm?id=359563]].

More information regarding vector clocks is available [[here|Vector clocks]].

h2. Backends

Sharing data among nodes, on rings, etc. is all well and good, but at some point, it has to actually be stored somewhere - like on disk! Because Riak is relevant to a wide variety of applications, its "backend" storage system is a pluggable one.

Each node may be configured with a different Erlang module for doing the simple storage, at the vnode level, below all of the interconnected cluster details.  At the backend level, a module only needs to define "get", "put", "delete", and "keys list" functions that receive a key and value which are both Erlang binaries.  The backend can consider these binaries completely opaque data, or examine them to make decisions about how best to store them.

The following backends are packaged with Riak:

# *riak_kv_bitcask_backend* - stores data to bitcask (_this is the default backend_)
# *riak_kv_fs_backend* - stores data directly to files in a nested directory structure on disk
# *riak_kv_ets_backend* - stores data in ETS tables (which makes it volatile storage, but great for debugging)
# *riak_kv_dets_backend* - stores data on-disk in DETS tables
# *riak_kv_gb_trees_backend* - stores data using Erlang gb_trees
# *riak_kv_cache_backend* - turns a bucket into a memcached-type memory cache, and ejects the least recently used objects either when the cache becomes full or the object's lease expires
# *riak_kv_multi_backend* - configure per-bucket backends

<div class="info">
The Innostore backend is not packaged with Riak for licensing reasons. See [[Setting Up Innostore]] if you would like to use this backend.
</div>

